=== Scheduling ===
1. SRTF is not used in production systems because it requires knowledge about
   the runtime of the processes. This is simply not possible in most
   circumstances, especially in any system that isn't embedded since a lot of
   processes may have varying completion times depending on their input.

2. Scenario where MLFQ is probably better than Lottery:
   If most of the processes that run switch between being IO or CPU bound over
   time, then MLFQ will perform better than lottery. MLFQ can easily adapt to
   the changes in the processes behaviors, since the design allows for a dynamic
   reassignment of priority depending on what that process is doing.
   A basic lottery scheduler cannot do this since processes are assigned some
   number of tickets at creation, and without knowing the process behavior
   changes before hand, there can be no dynamic reassignment. This means that
   for a process that was born IO bound but changed to be CPU bound over time
   will be chosen often, decreasing the responsiveness of the system.

   Scenario where Lottery is probably better than MLFQ:
   MLFQ without any modifications has a problem of starving lower priority
   processes under certain conditions. This can happen if there are a relatively
   small number of CPU bound processes and a lot of IO bound ones. The CPU
   processs will quickly be pushed to the lower queues, and will never get
   to run since there are so many processes that return from interrupts which
   place them in the top queue.
   Lottery scheduling doesn't have this problem so it'll not starve the CPU
   bound process, hence performing better.

3a. In real systems where context switches have a cost, FIFO is always going to
    have shorter average completion time than RR since it never has to switch
    from running a program to another.

3b. This can never happen. Let us consider the extremes.
    If RR had an extremely short quantum, the average start time will be almost
    nothing. So FIFO will have a worse average start time.
    If RR had an extremely long quantum (longer than the running time of any
    process), then it just reduces down to FIFO. This means that FIFO and RR
    has the same average start time.
    Therefore, there can never be a case where FIFO has a better average start
    time than RR.

3c. This can never happen. The problem depends on the cost of context switches.
    Let us examine the extreme cases once again.
    If the cost is nothing, then RR's completion time will be the same as FIFO's
    If the cost is very large, then RR's completion time will be much longer than
    FIFOs.
    There is not a case where RR's average completion time is faster since that
    would mean context switches somehow speed up processing.

3d. This will happen if the chosen quantum is smaller than the running time of
    at least one of the processes, and this process is not at the end of the
    queue. RR will cut off the long process, and allow the next to start, which
    leads to an improved average start time compared to FIFO.

3e. This can never happen.
    Since both RR and FIFO are running the same number of processes, the
    throughput is decided by the total completion time. This in turn is
    dependent on the cost of the context switch.
    If the cost is 0, the completion time is the same for RR and FIFO, so
    the throughputs are equal.
    If the cost is greater than 0, then RR will have a higher completion time
    which means a lower throughput.
    Since there is never a case where context switches have negative cost,
    RR will never have a higher throughput than FIFO

=== Deadlock ===
1a. on another sheet

1b. There is a cycle. It involves p1 and p3

1c. The system is not in deadlock.
    p2 finishes, and releases an R1.
    p1 obtains R1
    p1 is now able to finish
    p4 finishes, and releases an R2.
    p3 obtains R2
    p3 is now able to finish

2. Yes. The safe state is defined by whether or not all threads can eventually
   exit. The banker's algorithm looks all the way to the completion of all
   threads to determine if a state is safe.

3. We can allow resources to be preempted. To do so without disastrous
   consequences, the resource should be virtualized, but this is impossible for
   certain resources.

=== Memory ===
1a. Locate to the contiguous chunk that pertains to the process.
    If the contents of the memory has been modified, the whole thing should be
    written back to disk.
    Invalidate the blocks in memory previously used for the process in order to
    allow other processes to use it.
    Do the same for the registers that were used to perform dynamic relocation.

1b. Write to disk the updated contents for all of the dirty page table entries.
    Invalidate the previously used parts of the memory to make it  available for
    use.

1c. Find entries in the RLT that were being used by the process that exited.
    If the entry is marked as dirty, write it back to disk
    Mark the entry as available (invalid)
    There is probably a hash table that tries to make the process faster. Remove
    the hash table's members that correspond to the invalidated RLT entries.

    If a TLB is being used, write back dirty entries to RAM and invalidate all
    entries related to that process before proceeding with doing the above
    procedure to RAM.

2ai. TLB state:
     : x, x, x initial state
    0: 0, x, x compulsory miss, LRU: 0, MRU: 0
    1: 0, 1, x compulsory miss, LRU: 0, MRU: 1
    2: 0, 1, 2 compulsory miss, LRU: 0, MRU: 2
    3: 3, 1, 2 compulsory miss, LRU: 1, MRU: 3
    2: 3, 1, 2 TLB hit,         LRU: 1, MRU: 2
    4: 3, 4, 2 compulsory miss, LRU: 3, MRU: 4
    3: 3, 4, 2 TLB hit,         LRU: 2, MRU: 3
    1: 3, 4, 1 regular miss,    LRU: 4, MRU: 1
    1: 3, 4, 1 TLB hit,         LRU: 4, MRU: 1
    5: 3, 5, 1 compulsory miss, LRU: 3, MRU: 5
    2: 2, 5, 1 regular miss,    LRU: 1, MRU: 2
    4: 2, 5, 4 regular miss,    LRU: 5, MRU: 4
    6: 2, 6, 4 compulsory miss, LRU: 2, MRU: 6
    3, 3, 6, 4 regular miss,    LRU: 4, MRU: 3
    3: 3, 6, 4 TLB hit,         LRU: 4, MRU: 3
    4: 3, 6, 4 TLB hit,         LRU: 6, MRU: 4
    6: 3, 6, 4 TLB hit,         LRU: 3, MRU: 6
    3: 3, 6, 4 TLB hit,         LRU: 4, MRU: 3
    4: 3, 6, 4 TLB hit,         LRU: 6, MRU: 4
    7: 3, 7, 4 compulsory miss, LRU: 3, MRU: 7

    There are 4 non-compulsory misses

2aii: TLB state:
     : x, x, x   x, x, x  clock pointer: 0 initial state
    0: 0, x, x   y, x, x  clock pointer: 0 compulsory miss
    1: 0, 1, x   y, y, x  clock pointer: 0 compulsory miss
    2: 0, 1, 2   y, y, y  clock pointer: 0 compulsory miss
    3: 3, 1, 2   y, n, n  clock pointer: 0 compulsory miss: clock runs a full cycle, and evicts the first entry
    2: 3, 1, 2   y, n, y  clock pointer: 0 TLB hit, 2 marked as used
    4: 3, 4, 2   n, y, y  clock pointer: 1 compulsory miss: clock marks current as unused and moves over one to find an evictable entry
    3: 3, 4, 2   y, y, y  clock pointer: 1 TLB hit, 3 marked as used
    1: 3, 1, 2   n, y, n  clock pointer: 1 regular miss: clock runs a full cycle, and evicts the second entry
    1: 3, 1, 2   n, y, n  clock pointer: 1 TLB hit, 1 is already marked as used
    5: 3, 2, 5   n, n, y  clock pointer: 2 compulsory miss: clock marks current as unused and moves over one to find an evictable entry
    2: 3, 2, 5   n, y, y  clock pointer: 2 TLB hit, 2 marked as used
    4: 4, 2, 5   y, y, n  clock pointer: 0 regular miss: clock marks current as unused and moves over one to find an evictable entry
    6: 4, 2, 6   n, n, y  clock pointer: 2 compulsory miss: clock marks current and next entries as unused and evicts third entry
    3: 3, 2, 6   y, n, n  clock pointer: 0 regular miss: clock marks current as unused and moves over one to find an evictable entry
    3: 3, 2, 6   y, n, n  clock pointer: 0 TLB hit, 3 is already marked as used
    4: 3, 4, 6   n, y, n  clock pointer: 1 regular miss, clock marks current as unused and moves over one to find an evictable entry
    6: 3, 4, 6   n, y, y  clock pointer: 1 TLB hit, 6 marked as used
    3: 3, 4, 6   y, y, y  clock pointer: 1 TLB hit, 3 marked as used
    4: 3, 4, 6   y, y, y  clock pointer: 1 TLB hit, 4 marked as used
    7: 3, 7, 6   n, y, n  clock pointer: 1 compulsory miss: clock runs a full cycle, and evicts the second entry

    There are 4 non-compulsory misses


=== File Systems ===
1a. Assuming that we want whole blocks to be addressable, we need 11 bits for
    2k blocks. We can use the rest, 5 bits, for addressing blocks, which is
    2^5 = 32 blocks.

1b. With 32 bits per entry, and 2k in a block, the inode table can have up to
    64 entries.
    These 64 entries all map to another block of 2k each, which means it can
    address 131072 bytes.
    This means that both the table and the addressable space fits in the 1GB
    disk.


1c. We have 1gb / 1kb = 1,000,000 blocks in disk. This means that our FAT
    table needs to store a million entries.
    Each of these entires correspond to the physical block number, and also
    contains a pointer to the next block in the file.
    To be able to address a million entries, you will need 20 bits per entry.
    Assuming bit-level allocation is possible, the page table will be at least
    20,000,000 bits = 2,500,000 bytes = 2.5 MB

2a. If the block that comes after the contiguous allocation is free, then use
    that block.
    If it's being used by some other file, then some relocation will have to
    occur. One (not great) strategy is to move the proceeding file over by one
    block, moving following files if necessary, and repeating until the working
    file has enough room to continue.

2b. You should have some data structure that stores free blocks. Get the address
    of this block, and change the previously-last block's pointer to point to
    this new block. Go to the new block and write whatever you have to write.
    Set the new block's pointer to be -1, or repeat the process if further
    expansion is necessary.

2c. Iterate through each entry in the table until you find one that isn't being
    used. Have the previously-last entry point to this entry, and write contents
    in the physical block that corresponds to the new entry. Repeat if necessary
    and when the newly added block is the last that you need, set its pointer to
    be -1

2d. The inode table is specific to a file, so look at the table entry that
    directly follows the current last one. If it is free, find a free block in
    disk, write to it, and have the new entry point to it.

    If it is not free, navigate down the multiple levels until you do find an
    unused entry, then perform the allocation steps above.


=== Distributed Lock Acquisition ===
1a. Depends on who in the cycle is holding the token.
    The least: if the node holding the lock is directly before the requesting
    node in the cycle, then there is only one message. (the token being passed)
    The most: if the node holding the lock is directly after the requesting node
    in the cycle, then there are N messages since the token needs to be passed
    around the whole cycle before the requesting node can obtain it.

1b. The requesting node sends a request to all other nodes (N - 1 messages)
    Each of the other nodes then sends an acknowledgment message to the
    requesting node before it can obtain the lock (another N - 1 messages)
    Total sent: 2(N - 1) messages

2a. 0 messages. The node that was holding the token goes ahead and obtains
    the lock, and executes its critical section.

2b. Depends on when the nodes send and receives requests
    The least: Every single node in the network will send every other node a
    request message. This is N(N - 1) messages.
    If there is one node whose messages reach all other nodes and these nodes
    respond before , before any other messages reach any other node, This is
    N - 1 acknowledgement mesasges to this fast node.
    If all of the acks reach the fast node before any other message reaches any
    other node, then the fast node can execute its critical section after just
    (N + 1)(N - 1) mesasges in the network.

    The most: Every single node in the network will send every other node a
    request message. This is N(N - 1) messages.
    Every single node will receive these messages, and send either a positive
    or a negative acknowledgement message. This is one message per request so
    this is another N(N - 1) messages. This can happen if speeds between nodes
    are pretty balanced.
    Since the node that requested it first needs to get an acknowledgement
    from every other node, it will not be able to begin before all of these
    messages have been sent and received.
    Total sent: 2N(N - 1) messages
